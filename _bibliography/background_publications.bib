---
---
References
==========

@article{dwivediVisualFeaturesAre2024,
	title = {Visual features are processed before navigational affordances in the human brain},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-55652-y},
	doi = {10.1038/s41598-024-55652-y},
	abstract = {To navigate through their immediate environment humans process scene information rapidly. How does the cascade of neural processing elicited by scene viewing to facilitate navigational planning unfold over time? To investigate, we recorded human brain responses to visual scenes with electroencephalography and related those to computational models that operationalize three aspects of scene processing (2D, 3D, and semantic information), as well as to a behavioral model capturing navigational affordances. We found a temporal processing hierarchy: navigational affordance is processed later than the other scene features (2D, 3D, and semantic) investigated. This reveals the temporal order with which the human brain computes complex scene information and suggests that the brain leverages these pieces of information to plan navigation.},
	language = {en},
	number = {1},
	urldate = {2024-07-01},
	journal = {Scientific Reports},
	author = {Dwivedi, Kshitij and Sadiya, Sari and Balode, Marta P. and Roig, Gemma and Cichy, Radoslaw M.},
	month = mar,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Psychology},
	pages = {5573},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\ECQFWCVK\\Dwivedi et al. - 2024 - Visual features are processed before navigational .pdf:application/pdf},
}

@misc{ernstSelfSupervisedLearningColor2024,
	title = {Self-{Supervised} {Learning} of {Color} {Constancy}},
	url = {http://arxiv.org/abs/2404.08127},
	abstract = {Color constancy (CC) describes the ability of the visual system to perceive an object as having a relatively constant color despite changes in lighting conditions. While CC and its limitations have been carefully characterized in humans, it is still unclear how the visual system acquires this ability during development. Here, we present a first study showing that CC develops in a neural network trained in a self-supervised manner through an invariance learning objective. During learning, objects are presented under changing illuminations, while the network aims to map subsequent views of the same object onto close-by latent representations. This gives rise to representations that are largely invariant to the illumination conditions, offering a plausible example of how CC could emerge during human cognitive development via a form of self-supervised learning.},
	language = {en},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Ernst, Markus R. and López, Francisco M. and Aubret, Arthur and Fleming, Roland W. and Triesch, Jochen},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08127 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 7 pages, 5 figures, submitted to the IEEE International Conference on Development and Learning (ICDL 2024)},
	annote = {Comment: 7 pages, 5 figures, submitted to the IEEE International Conference on Development and Learning (ICDL 2024)},
	annote = {Comment: 7 pages, 5 figures, submitted to the IEEE International Conference on Development and Learning (ICDL 2024)},
	file = {2404.pdf:C\:\\Users\\felix\\Zotero\\storage\\TSYSWQ9R\\2404.pdf:application/pdf},
}

@article{sassenhagenTracesMeaningItself2020,
	title = {Traces of {Meaning} {Itself}: {Encoding} {Distributional} {Word} {Vectors} in {Brain} {Activity}},
	volume = {1},
	issn = {2641-4368},
	shorttitle = {Traces of {Meaning} {Itself}},
	url = {https://doi.org/10.1162/nol_a_00003},
	doi = {10.1162/nol_a_00003},
	abstract = {How is semantic information stored in the human mind and brain? Some philosophers and cognitive scientists argue for vectorial representations of concepts, where the meaning of a word is represented as its position in a high-dimensional neural state space. At the intersection of natural language processing and artificial intelligence, a class of very successful distributional word vector models has developed that can account for classic EEG findings of language, that is, the ease versus difficulty of integrating a word with its sentence context. However, models of semantics have to account not only for context-based word processing, but should also describe how word meaning is represented. Here, we investigate whether distributional vector representations of word meaning can model brain activity induced by words presented without context. Using EEG activity (event-related brain potentials) collected while participants in two experiments (English and German) read isolated words, we encoded and decoded word vectors taken from the family of prediction-based Word2vec algorithms. We found that, first, the position of a word in vector space allows the prediction of the pattern of corresponding neural activity over time, in particular during a time window of 300 to 500 ms after word onset. Second, distributional models perform better than a human-created taxonomic baseline model (WordNet), and this holds for several distinct vector-based models. Third, multiple latent semantic dimensions of word meaning can be decoded from brain activity. Combined, these results suggest that empiricist, prediction-based vectorial representations of meaning are a viable candidate for the representational architecture of human semantic knowledge.},
	number = {1},
	urldate = {2024-07-01},
	journal = {Neurobiology of Language},
	author = {Sassenhagen, Jona and Fiebach, Christian J.},
	month = mar,
	year = {2020},
	pages = {54--76},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\PVMI7PXW\\Sassenhagen und Fiebach - 2020 - Traces of Meaning Itself Encoding Distributional .pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\CSAP8USW\\Traces-of-Meaning-Itself-Encoding-Distributional.html:text/html},
}

@misc{berschNet2BrainToolboxCompare2022,
	title = {{Net2Brain}: {A} {Toolbox} to compare artificial vision models with human brain responses},
	shorttitle = {{Net2Brain}},
	url = {http://arxiv.org/abs/2208.09677},
	doi = {10.48550/arXiv.2208.09677},
	abstract = {We introduce Net2Brain, a graphical and command-line user interface toolbox for comparing the representational spaces of artificial deep neural networks (DNNs) and human brain recordings. While different toolboxes facilitate only single functionalities or only focus on a small subset of supervised image classification models, Net2Brain allows the extraction of activations of more than 600 DNNs trained to perform a diverse range of vision-related tasks (e.g semantic segmentation, depth estimation, action recognition, etc.), over both image and video datasets. The toolbox computes the representational dissimilarity matrices (RDMs) over those activations and compares them to brain recordings using representational similarity analysis (RSA), weighted RSA, both in specific ROIs and with searchlight search. In addition, it is possible to add a new data set of stimuli and brain recordings to the toolbox for evaluation. We demonstrate the functionality and advantages of Net2Brain with an example showcasing how it can be used to test hypotheses of cognitive computational neuroscience.},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Bersch, Domenic and Dwivedi, Kshitij and Vilas, Martina and Cichy, Radoslaw M. and Roig, Gemma},
	month = aug,
	year = {2022},
	note = {arXiv:2208.09677 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 4 Pages, 3 figures, submitted and accepted to CCNeuro 2022. For associated repository, see https://github.com/ToastyDom/Net2Brain Update 1: Changed Citation},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\XJ6BRPEW\\Bersch et al. - 2022 - Net2Brain A Toolbox to compare artificial vision .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\42KCDVPV\\2208.html:text/html},
}

@article{tonevaCombiningComputationalControls2022,
	title = {Combining computational controls with natural text reveals aspects of meaning composition},
	volume = {2},
	copyright = {2022 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {2662-8457},
	url = {https://www.nature.com/articles/s43588-022-00354-6},
	doi = {10.1038/s43588-022-00354-6},
	abstract = {To study a core component of human intelligence—our ability to combine the meaning of words—neuroscientists have looked to linguistics. However, linguistic theories are insufficient to account for all brain responses reflecting linguistic composition. In contrast, we adopt a data-driven approach to study the composed meaning of words beyond their individual meaning, which we term ‘supra-word meaning’. We construct a computational representation for supra-word meaning and study its brain basis through brain recordings from two complementary imaging modalities. Using functional magnetic resonance imaging, we reveal that hubs that are thought to process lexical meaning also maintain supra-word meaning, suggesting a common substrate for lexical and combinatorial semantics. Surprisingly, we cannot detect supra-word meaning in magnetoencephalography, which suggests that composed meaning might be maintained through a different neural mechanism than the synchronized firing of pyramidal cells. This sensitivity difference has implications for past neuroimaging results and future wearable neurotechnology.},
	language = {en},
	number = {11},
	urldate = {2024-07-01},
	journal = {Nature Computational Science},
	author = {Toneva, Mariya and Mitchell, Tom M. and Wehbe, Leila},
	month = nov,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Language, Neural encoding},
	pages = {745--757},
	file = {Akzeptierte Version:C\:\\Users\\felix\\Zotero\\storage\\L8I4366D\\Toneva et al. - 2022 - Combining computational controls with natural text.pdf:application/pdf},
}

@article{dwivediUnveilingFunctionsVisual2021,
	title = {Unveiling functions of the visual cortex using task-specific deep neural networks},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009267},
	doi = {10.1371/journal.pcbi.1009267},
	abstract = {The human visual cortex enables visual perception through a cascade of hierarchical computations in cortical regions with distinct functionalities. Here, we introduce an AI-driven approach to discover the functional mapping of the visual cortex. We related human brain responses to scene images measured with functional MRI (fMRI) systematically to a diverse set of deep neural networks (DNNs) optimized to perform different scene perception tasks. We found a structured mapping between DNN tasks and brain regions along the ventral and dorsal visual streams. Low-level visual tasks mapped onto early brain regions, 3-dimensional scene perception tasks mapped onto the dorsal stream, and semantic tasks mapped onto the ventral stream. This mapping was of high fidelity, with more than 60\% of the explainable variance in nine key regions being explained. Together, our results provide a novel functional mapping of the human visual cortex and demonstrate the power of the computational approach.},
	language = {en},
	number = {8},
	urldate = {2024-07-01},
	journal = {PLOS Computational Biology},
	author = {Dwivedi, Kshitij and Bonner, Michael F. and Cichy, Radoslaw Martin and Roig, Gemma},
	month = aug,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Functional magnetic resonance imaging, Linear regression analysis, Neural networks, Permutation, Semantics, Sensory perception, Vision, Visual cortex},
	pages = {e1009267},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\NX3RJWGC\\Dwivedi et al. - 2021 - Unveiling functions of the visual cortex using tas.pdf:application/pdf},
}

@article{dwivediUnravelingRepresentationsSceneselective2021,
	title = {Unraveling {Representations} in {Scene}-selective {Brain} {Regions} {Using} {Scene}-{Parsing} {Deep} {Neural} {Networks}},
	volume = {33},
	issn = {0898-929X},
	url = {https://doi.org/10.1162/jocn_a_01624},
	doi = {10.1162/jocn_a_01624},
	abstract = {Visual scene perception is mediated by a set of cortical regions that respond preferentially to images of scenes, including the occipital place area (OPA) and parahippocampal place area (PPA). However, the differential contribution of OPA and PPA to scene perception remains an open research question. In this study, we take a deep neural network (DNN)-based computational approach to investigate the differences in OPA and PPA function. In a first step, we search for a computational model that predicts fMRI responses to scenes in OPA and PPA well. We find that DNNs trained to predict scene components (e.g., wall, ceiling, floor) explain higher variance uniquely in OPA and PPA than a DNN trained to predict scene category (e.g., bathroom, kitchen, office). This result is robust across several DNN architectures. On this basis, we then determine whether particular scene components predicted by DNNs differentially account for unique variance in OPA and PPA. We find that variance in OPA responses uniquely explained by the navigation-related floor component is higher compared to the variance explained by the wall and ceiling components. In contrast, PPA responses are better explained by the combination of wall and floor, that is, scene components that together contain the structure and texture of the scene. This differential sensitivity to scene components suggests differential functions of OPA and PPA in scene processing. Moreover, our results further highlight the potential of the proposed computational approach as a general tool in the investigation of the neural basis of human scene perception.},
	number = {10},
	urldate = {2024-07-01},
	journal = {Journal of Cognitive Neuroscience},
	author = {Dwivedi, Kshitij and Cichy, Radoslaw Martin and Roig, Gemma},
	month = sep,
	year = {2021},
	pages = {2032--2043},
	file = {Akzeptierte Version:C\:\\Users\\felix\\Zotero\\storage\\Q7NNH46Z\\Dwivedi et al. - 2021 - Unraveling Representations in Scene-selective Brai.pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\UK7HI79F\\Unraveling-Representations-in-Scene-selective.html:text/html},
}

@misc{tonevaInterpretingImprovingNaturallanguage2019,
	title = {Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
	url = {https://arxiv.org/abs/1905.11833v4},
	abstract = {Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination.},
	language = {en},
	urldate = {2024-07-01},
	journal = {arXiv.org},
	author = {Toneva, Mariya and Wehbe, Leila},
	month = may,
	year = {2019},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\V2XIL34E\\Toneva und Wehbe - 2019 - Interpreting and improving natural-language proces.pdf:application/pdf},
}

@misc{schwartzInducingBrainrelevantBias2019,
	title = {Inducing brain-relevant bias in natural language processing models},
	url = {http://arxiv.org/abs/1911.03268},
	doi = {10.48550/arXiv.1911.03268},
	abstract = {Progress in natural language processing (NLP) models that estimate representations of word sequences has recently been leveraged to improve the understanding of language processing in the brain. However, these models have not been specifically designed to capture the way the brain represents language meaning. We hypothesize that fine-tuning these models to predict recordings of brain activity of people reading text will lead to representations that encode more brain-activity-relevant language information. We demonstrate that a version of BERT, a recently introduced and powerful language model, can improve the prediction of brain activity after fine-tuning. We show that the relationship between language and brain activity learned by BERT during this fine-tuning transfers across multiple participants. We also show that, for some participants, the fine-tuned representations learned from both magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) are better for predicting fMRI than the representations learned from fMRI alone, indicating that the learned representations capture brain-activity-relevant information that is not simply an artifact of the modality. While changes to language representations help the model predict brain activity, they also do not harm the model's ability to perform downstream NLP tasks. Our findings are notable for research on language understanding in the brain.},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Schwartz, Dan and Toneva, Mariya and Wehbe, Leila},
	month = oct,
	year = {2019},
	note = {arXiv:1911.03268 [cs, q-bio]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: To be published in the proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\WL2AXIT9\\Schwartz et al. - 2019 - Inducing brain-relevant bias in natural language p.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\S57P6AF5\\1911.html:text/html},
}

@misc{aubretLearningObjectSemantic2024,
	title = {Learning {Object} {Semantic} {Similarity} with {Self}-{Supervision}},
	url = {http://arxiv.org/abs/2405.05143},
	doi = {10.48550/arXiv.2405.05143},
	abstract = {Humans judge the similarity of two objects not just based on their visual appearance but also based on their semantic relatedness. However, it remains unclear how humans learn about semantic relationships between objects and categories. One important source of semantic knowledge is that semantically related objects frequently co-occur in the same context. For instance, forks and plates are perceived as similar, at least in part, because they are often experienced together in a ``kitchen" or ``eating'' context. Here, we investigate whether a bio-inspired learning principle exploiting such co-occurrence statistics suffices to learn a semantically structured object representation \{{\textbackslash}em de novo\} from raw visual or combined visual and linguistic input. To this end, we simulate temporal sequences of visual experience by binding together short video clips of real-world scenes showing objects in different contexts. A bio-inspired neural network model aligns close-in-time visual representations while also aligning visual and category label representations to simulate visuo-language alignment. Our results show that our model clusters object representations based on their context, e.g. kitchen or bedroom, in particular in high-level layers of the network, akin to humans. In contrast, lower-level layers tend to better reflect object identity or category. To achieve this, the model exploits two distinct strategies: the visuo-language alignment ensures that different objects of the same category are represented similarly, whereas the temporal alignment leverages that objects from the same context are frequently seen in succession to make their representations more similar. Overall, our work suggests temporal and visuo-language alignment as plausible computational principles for explaining the origins of certain forms of semantic knowledge in humans.},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Aubret, Arthur and Schaumlöffel, Timothy and Roig, Gemma and Triesch, Jochen},
	month = apr,
	year = {2024},
	note = {arXiv:2405.05143 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\X3EFC5TM\\Aubret et al. - 2024 - Learning Object Semantic Similarity with Self-Supe.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\C43SMCQ9\\2405.html:text/html},
}

---
---
References
==========

@online{aubretLearningObjectSemantic2024,
  title = {Learning {{Object Semantic Similarity}} with {{Self-Supervision}}},
  author = {Aubret, Arthur and Schaumlöffel, Timothy and Roig, Gemma and Triesch, Jochen},
  date = {2024-04-19},
  series = {Proceedings of the 2024 {{IEEE International Conference}} on {{Development}} and {{Learning}} ({{ICDL}})},
  eprint = {2405.05143},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2405.05143},
  url = {http://arxiv.org/abs/2405.05143},
  urldate = {2024-07-01},
  abstract = {Humans judge the similarity of two objects not just based on their visual appearance but also based on their semantic relatedness. However, it remains unclear how humans learn about semantic relationships between objects and categories. One important source of semantic knowledge is that semantically related objects frequently co-occur in the same context. For instance, forks and plates are perceived as similar, at least in part, because they are often experienced together in a ``kitchen" or ``eating'' context. Here, we investigate whether a bio-inspired learning principle exploiting such co-occurrence statistics suffices to learn a semantically structured object representation \{\textbackslash em de novo\} from raw visual or combined visual and linguistic input. To this end, we simulate temporal sequences of visual experience by binding together short video clips of real-world scenes showing objects in different contexts. A bio-inspired neural network model aligns close-in-time visual representations while also aligning visual and category label representations to simulate visuo-language alignment. Our results show that our model clusters object representations based on their context, e.g. kitchen or bedroom, in particular in high-level layers of the network, akin to humans. In contrast, lower-level layers tend to better reflect object identity or category. To achieve this, the model exploits two distinct strategies: the visuo-language alignment ensures that different objects of the same category are represented similarly, whereas the temporal alignment leverages that objects from the same context are frequently seen in succession to make their representations more similar. Overall, our work suggests temporal and visuo-language alignment as plausible computational principles for explaining the origins of certain forms of semantic knowledge in humans.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\felix\\Zotero\\storage\\X3EFC5TM\\Aubret et al. - 2024 - Learning Object Semantic Similarity with Self-Supe.pdf;C\:\\Users\\felix\\Zotero\\storage\\C43SMCQ9\\2405.html}
}

@online{ernstSelfSupervisedLearningColor2024,
  title = {Self-{{Supervised Learning}} of {{Color Constancy}}},
  author = {Ernst, Markus R. and López, Francisco M. and Aubret, Arthur and Fleming, Roland W. and Triesch, Jochen},
  date = {2024-04-11},
  series = {Proceedings of the 2024 {{IEEE International Conference}} on {{Development}} and {{Learning}} ({{ICDL}})},
  eprint = {2404.08127},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2404.08127},
  urldate = {2024-07-01},
  abstract = {Color constancy (CC) describes the ability of the visual system to perceive an object as having a relatively constant color despite changes in lighting conditions. While CC and its limitations have been carefully characterized in humans, it is still unclear how the visual system acquires this ability during development. Here, we present a first study showing that CC develops in a neural network trained in a self-supervised manner through an invariance learning objective. During learning, objects are presented under changing illuminations, while the network aims to map subsequent views of the same object onto close-by latent representations. This gives rise to representations that are largely invariant to the illumination conditions, offering a plausible example of how CC could emerge during human cognitive development via a form of self-supervised learning.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C:\Users\felix\Zotero\storage\TSYSWQ9R\2404.pdf}
}

@article{lahnerModelingShortVisual2024,
  title = {Modeling Short Visual Events through the {{BOLD}} Moments Video {{fMRI}} Dataset and Metadata},
  author = {Lahner, Benjamin and Dwivedi, Kshitij and Iamshchinina, Polina and Graumann, Monika and Lascelles, Alex and Roig, Gemma and Gifford, Alessandro Thomas and Pan, Bowen and Jin, SouYoung and Ratan Murty, N. Apurva and Kay, Kendrick and Oliva, Aude and Cichy, Radoslaw},
  date = {2024-07-24},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {15},
  number = {1},
  pages = {6241},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-50310-3},
  url = {https://www.nature.com/articles/s41467-024-50310-3},
  urldate = {2024-07-30},
  abstract = {Studying the neural basis of human dynamic visual perception requires extensive experimental data to evaluate the large swathes of functionally diverse brain neural networks driven by perceiving visual events. Here, we introduce the BOLD Moments Dataset (BMD), a repository of whole-brain fMRI responses to over 1000 short (3\,s) naturalistic video clips of visual events across ten human subjects. We use the videos’ extensive metadata to show how the brain represents word- and sentence-level descriptions of visual events and identify correlates of video memorability scores extending into the parietal cortex. Furthermore, we reveal a match in hierarchical processing between cortical regions of interest and video-computable deep neural networks, and we showcase that BMD successfully captures temporal dynamics of visual events at second resolution. With its rich metadata, BMD offers new perspectives and accelerates research on the human brain basis of visual event perception.},
  langid = {english},
  keywords = {Neural encoding,Perception,Visual system},
  file = {C:\Users\felix\Zotero\storage\MLQBFTHX\Lahner et al. - 2024 - Modeling short visual events through the BOLD mome.pdf}
}

@article{ootaJointProcessingLinguistic2023a,
  title = {Joint Processing of Linguistic Properties in Brains and Language Models},
  author = {Oota, Subbareddy and Gupta, Manish and Toneva, Mariya},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {18001--18014},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/3a0e2de215bd17c39ad08ba1d16c1b12-Abstract-Conference.html},
  urldate = {2024-07-30},
  langid = {english},
  file = {C:\Users\felix\Zotero\storage\QGZCWPRS\Oota et al. - 2023 - Joint processing of linguistic properties in brain.pdf}
}

@online{ootaSpeechLanguageModels2024,
  title = {Speech Language Models Lack Important Brain-Relevant Semantics},
  author = {Oota, Subba Reddy and Çelik, Emin and Deniz, Fatma and Toneva, Mariya},
  date = {2024-06-16},
  eprint = {2311.04664},
  eprinttype = {arXiv},
  eprintclass = {cs, eess, q-bio},
  doi = {10.48550/arXiv.2311.04664},
  url = {http://arxiv.org/abs/2311.04664},
  urldate = {2024-07-01},
  abstract = {Despite known differences between reading and listening in the brain, recent work has shown that text-based language models predict both text-evoked and speech-evoked brain activity to an impressive degree. This poses the question of what types of information language models truly predict in the brain. We investigate this question via a direct approach, in which we systematically remove specific low-level stimulus features (textual, speech, and visual) from language model representations to assess their impact on alignment with fMRI brain recordings during reading and listening. Comparing these findings with speech-based language models reveals starkly different effects of low-level features on brain alignment. While text-based models show reduced alignment in early sensory regions post-removal, they retain significant predictive power in late language regions. In contrast, speech-based models maintain strong alignment in early auditory regions even after feature removal but lose all predictive power in late language regions. These results suggest that speech-based models provide insights into additional information processed by early auditory regions, but caution is needed when using them to model processing in late language regions. We make our code publicly available. [https://github.com/subbareddy248/speech-llm-brain]},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Quantitative Biology - Neurons and Cognition},
  file = {C\:\\Users\\felix\\Zotero\\storage\\2W5MRDIE\\Oota et al. - 2024 - Speech language models lack important brain-releva.pdf;C\:\\Users\\felix\\Zotero\\storage\\SCRZEMZV\\2311.html}
}

@inproceedings{schaumloffelCaregiverTalkShapes2023,
  title = {Caregiver {{Talk Shapes Toddler Vision}}: {{A Computational Study}} of {{Dyadic Play}}},
  shorttitle = {Caregiver {{Talk Shapes Toddler Vision}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Development}} and {{Learning}} ({{ICDL}})},
  author = {Schaumlöffel, Timothy and Aubret, Arthur and Roig, Gemma and Triesch, Jochen},
  date = {2023-11-09},
  eprint = {2312.04118},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {67--72},
  doi = {10.1109/ICDL55364.2023.10364409},
  url = {http://arxiv.org/abs/2312.04118},
  urldate = {2024-07-01},
  abstract = {Infants' ability to recognize and categorize objects develops gradually. The second year of life is marked by both the emergence of more semantic visual representations and a better understanding of word meaning. This suggests that language input may play an important role in shaping visual representations. However, even in suitable contexts for word learning like dyadic play sessions, caregivers utterances are sparse and ambiguous, often referring to objects that are different from the one to which the child attends. Here, we systematically investigate to what extent caregivers' utterances can nevertheless enhance visual representations. For this we propose a computational model of visual representation learning during dyadic play. We introduce a synthetic dataset of ego-centric images perceived by a toddler-agent that moves and rotates toy objects in different parts of its home environment while hearing caregivers' utterances, modeled as captions. We propose to model toddlers' learning as simultaneously aligning representations for 1) close-in-time images and 2) co-occurring images and utterances. We show that utterances with statistics matching those of real caregivers give rise to representations supporting improved category recognition. Our analysis reveals that a small decrease/increase in object-relevant naming frequencies can drastically impact the learned representations. This affects the attention on object names within an utterance, which is required for efficient visuo-linguistic alignment. Overall, our results support the hypothesis that caregivers' naming utterances can improve toddlers' visual representations.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {C\:\\Users\\felix\\Zotero\\storage\\AV6UR8J8\\Schaumlöffel et al. - 2023 - Caregiver Talk Shapes Toddler Vision A Computatio.pdf;C\:\\Users\\felix\\Zotero\\storage\\WDLDI6PZ\\2312.html}
}

@article{schaumloffelPEACSPREFIXENCODING2023,
  title = {{{PEACS}}: {{PREFIX ENCODING FOR AUDITORY CAPTION SYNTHESIS}}},
  author = {Schaumlöffel, Timothy and Vilas, Martina G. and Roig, Gemma},
  date = {2023},
  journaltitle = {IEEE Transactions on Multimedia},
  shortjournal = {IEEE Trans. Multimedia},
  volume = {17},
  number = {10},
  pages = {1733--1746},
  issn = {1520-9210, 1941-0077},
  doi = {https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf},
  url = {http://ieeexplore.ieee.org/document/7100934/},
  urldate = {2024-07-01},
  abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder’s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.},
  langid = {english},
  file = {C:\Users\felix\Zotero\storage\3YDUBNF3\Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf}
}

@article{vilasAnalyzingVisionTransformers2023a,
  title = {Analyzing {{Vision Transformers}} for {{Image Classification}} in {{Class Embedding Space}}},
  author = {Vilas, Martina G. and Schaumlöffel, Timothy and Roig, Gemma},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {40030--40041},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/7dd309df03d37643b96f5048b44da798-Abstract-Conference.html},
  urldate = {2024-07-30},
  langid = {english},
  file = {C:\Users\felix\Zotero\storage\KML6KDAY\Vilas et al. - 2023 - Analyzing Vision Transformers for Image Classifica.pdf}
}

@inproceedings{vilasPositionInnerInterpretability2024,
  title = {Position: {{An Inner Interpretability Framework}} for {{AI Inspired}} by {{Lessons}} from {{Cognitive Neuroscience}}},
  shorttitle = {Position},
  author = {Vilas, Martina G. and Adolfi, Federico and Poeppel, David and Roig, Gemma},
  date = {2024-06-06},
  url = {https://openreview.net/forum?id=66KmnMhGU5},
  urldate = {2024-07-30},
  abstract = {Inner Interpretability is a promising emerging field tasked with uncovering the inner mechanisms of AI systems, though how to develop these mechanistic theories is still much debated. Moreover, recent critiques raise issues that question its usefulness to advance the broader goals of AI. However, it has been overlooked that these issues resemble those that have been grappled with in another field: Cognitive Neuroscience. Here we draw the relevant connections and highlight lessons that can be transferred productively between fields. Based on these, we propose a general conceptual framework and give concrete methodological strategies for building mechanistic explanations in AI inner interpretability research. With this conceptual framework, Inner Interpretability can fend off critiques and position itself on a productive path to explain AI systems.},
  eventtitle = {Forty-First {{International Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {C:\Users\felix\Zotero\storage\TT78LFKT\Vilas et al. - 2024 - Position An Inner Interpretability Framework for .pdf}
}

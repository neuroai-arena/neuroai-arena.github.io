
@article{dwivediVisualFeaturesAre2024,
	title = {Visual features are processed before navigational affordances in the human brain},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-55652-y},
	doi = {10.1038/s41598-024-55652-y},
	abstract = {To navigate through their immediate environment humans process scene information rapidly. How does the cascade of neural processing elicited by scene viewing to facilitate navigational planning unfold over time? To investigate, we recorded human brain responses to visual scenes with electroencephalography and related those to computational models that operationalize three aspects of scene processing (2D, 3D, and semantic information), as well as to a behavioral model capturing navigational affordances. We found a temporal processing hierarchy: navigational affordance is processed later than the other scene features (2D, 3D, and semantic) investigated. This reveals the temporal order with which the human brain computes complex scene information and suggests that the brain leverages these pieces of information to plan navigation.},
	language = {en},
	number = {1},
	urldate = {2024-07-01},
	journal = {Scientific Reports},
	author = {Dwivedi, Kshitij and Sadiya, Sari and Balode, Marta P. and Roig, Gemma and Cichy, Radoslaw M.},
	month = mar,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Neuroscience, Psychology},
	pages = {5573},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\ECQFWCVK\\Dwivedi et al. - 2024 - Visual features are processed before navigational .pdf:application/pdf},
}

@misc{vilasAnalyzingVisionTransformers2023,
	title = {Analyzing {Vision} {Transformers} for {Image} {Classification} in {Class} {Embedding} {Space}},
	url = {http://arxiv.org/abs/2310.18969},
	abstract = {Despite the growing use of transformer models in computer vision, a mechanistic understanding of these networks is still needed. This work introduces a method to reverse-engineer Vision Transformers trained to solve image classification tasks. Inspired by previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research.},
	language = {en},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Vilas, Martina G. and Schaumlöffel, Timothy and Roig, Gemma},
	month = oct,
	year = {2023},
	note = {arXiv:2310.18969 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: NeurIPS 2023},
	annote = {Comment: NeurIPS 2023},
	annote = {Comment: NeurIPS 2023},
	file = {Vilas et al. - 2023 - Analyzing Vision Transformers for Image Classifica.pdf:C\:\\Users\\felix\\Zotero\\storage\\TAHDPDXQ\\Vilas et al. - 2023 - Analyzing Vision Transformers for Image Classifica.pdf:application/pdf},
}

@misc{ootaJointProcessingLinguistic2023,
	title = {Joint processing of linguistic properties in brains and language models},
	url = {http://arxiv.org/abs/2212.08094},
	abstract = {Language models have been shown to be very effective in predicting brain recordings of subjects experiencing complex language stimuli. For a deeper understanding of this alignment, it is important to understand the correspondence between the detailed processing of linguistic information by the human brain versus language models. We investigate this correspondence via a direct approach, in which we eliminate information related to specific linguistic properties in the language model representations and observe how this intervention affects the alignment with fMRI brain recordings obtained while participants listened to a story. We investigate a range of linguistic properties (surface, syntactic, and semantic) and find that the elimination of each one results in a significant decrease in brain alignment. Specifically, we find that syntactic properties (i.e. Top Constituents and Tree Depth) have the largest effect on the trend of brain alignment across model layers. These findings provide clear evidence for the role of specific linguistic information in the alignment between brain and language models, and open new avenues for mapping the joint information processing in both systems. We make the code publicly available1.},
	language = {en},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Oota, Subba Reddy and Gupta, Manish and Toneva, Mariya},
	month = nov,
	year = {2023},
	note = {arXiv:2212.08094 [cs, q-bio]},
	keywords = {Computer Science - Computation and Language, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 22 pages, 12 figures, To be published in the proceedings of the 37th Conference on Neural Information Processing Systems (NeurIPS 2023), New Orleans, USA},
	file = {Oota et al. - 2023 - Joint processing of linguistic properties in brain.pdf:C\:\\Users\\felix\\Zotero\\storage\\UPL8EE3Z\\Oota et al. - 2023 - Joint processing of linguistic properties in brain.pdf:application/pdf},
}

@inproceedings{schaumloffelCaregiverTalkShapes2023,
	title = {Caregiver {Talk} {Shapes} {Toddler} {Vision}: {A} {Computational} {Study} of {Dyadic} {Play}},
	shorttitle = {Caregiver {Talk} {Shapes} {Toddler} {Vision}},
	url = {http://arxiv.org/abs/2312.04118},
	doi = {10.1109/ICDL55364.2023.10364409},
	abstract = {Infants' ability to recognize and categorize objects develops gradually. The second year of life is marked by both the emergence of more semantic visual representations and a better understanding of word meaning. This suggests that language input may play an important role in shaping visual representations. However, even in suitable contexts for word learning like dyadic play sessions, caregivers utterances are sparse and ambiguous, often referring to objects that are different from the one to which the child attends. Here, we systematically investigate to what extent caregivers' utterances can nevertheless enhance visual representations. For this we propose a computational model of visual representation learning during dyadic play. We introduce a synthetic dataset of ego-centric images perceived by a toddler-agent that moves and rotates toy objects in different parts of its home environment while hearing caregivers' utterances, modeled as captions. We propose to model toddlers' learning as simultaneously aligning representations for 1) close-in-time images and 2) co-occurring images and utterances. We show that utterances with statistics matching those of real caregivers give rise to representations supporting improved category recognition. Our analysis reveals that a small decrease/increase in object-relevant naming frequencies can drastically impact the learned representations. This affects the attention on object names within an utterance, which is required for efficient visuo-linguistic alignment. Overall, our results support the hypothesis that caregivers' naming utterances can improve toddlers' visual representations.},
	urldate = {2024-07-01},
	booktitle = {2023 {IEEE} {International} {Conference} on {Development} and {Learning} ({ICDL})},
	author = {Schaumlöffel, Timothy and Aubret, Arthur and Roig, Gemma and Triesch, Jochen},
	month = nov,
	year = {2023},
	note = {arXiv:2312.04118 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {67--72},
	annote = {Comment: Proceedings of the 2023 IEEE International Conference on Development and Learning (ICDL)},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\AV6UR8J8\\Schaumlöffel et al. - 2023 - Caregiver Talk Shapes Toddler Vision A Computatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\WDLDI6PZ\\2312.html:text/html},
}

@article{schaumloffelPEACSPREFIXENCODING2023,
	title = {{PEACS}: {PREFIX} {ENCODING} {FOR} {AUDITORY} {CAPTION} {SYNTHESIS}},
	volume = {17},
	copyright = {https://creativecommons.org/licenses/by/3.0/legalcode},
	issn = {1520-9210, 1941-0077},
	url = {http://ieeexplore.ieee.org/document/7100934/},
	doi = {10.1109/TMM.2015.2428998},
	abstract = {This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder’s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset.},
	language = {en},
	number = {10},
	urldate = {2024-07-01},
	journal = {IEEE Transactions on Multimedia},
	author = {Schaumlöffel, Timothy and Vilas, Martina G. and Roig, Gemma},
	year = {2023},
	pages = {1733--1746},
	file = {Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf:C\:\\Users\\felix\\Zotero\\storage\\3YDUBNF3\\Stowell et al. - 2015 - Detection and Classification of Acoustic Scenes an.pdf:application/pdf},
}

@misc{ernstSelfSupervisedLearningColor2024,
	title = {Self-{Supervised} {Learning} of {Color} {Constancy}},
	url = {http://arxiv.org/abs/2404.08127},
	abstract = {Color constancy (CC) describes the ability of the visual system to perceive an object as having a relatively constant color despite changes in lighting conditions. While CC and its limitations have been carefully characterized in humans, it is still unclear how the visual system acquires this ability during development. Here, we present a first study showing that CC develops in a neural network trained in a self-supervised manner through an invariance learning objective. During learning, objects are presented under changing illuminations, while the network aims to map subsequent views of the same object onto close-by latent representations. This gives rise to representations that are largely invariant to the illumination conditions, offering a plausible example of how CC could emerge during human cognitive development via a form of self-supervised learning.},
	language = {en},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Ernst, Markus R. and López, Francisco M. and Aubret, Arthur and Fleming, Roland W. and Triesch, Jochen},
	month = apr,
	year = {2024},
	note = {arXiv:2404.08127 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 7 pages, 5 figures, submitted to the IEEE International Conference on Development and Learning (ICDL 2024)},
	annote = {Comment: 7 pages, 5 figures, submitted to the IEEE International Conference on Development and Learning (ICDL 2024)},
	annote = {Comment: 7 pages, 5 figures, submitted to the IEEE International Conference on Development and Learning (ICDL 2024)},
	file = {2404.pdf:C\:\\Users\\felix\\Zotero\\storage\\TSYSWQ9R\\2404.pdf:application/pdf},
}

@article{sassenhagenTracesMeaningItself2020,
	title = {Traces of {Meaning} {Itself}: {Encoding} {Distributional} {Word} {Vectors} in {Brain} {Activity}},
	volume = {1},
	issn = {2641-4368},
	shorttitle = {Traces of {Meaning} {Itself}},
	url = {https://doi.org/10.1162/nol_a_00003},
	doi = {10.1162/nol_a_00003},
	abstract = {How is semantic information stored in the human mind and brain? Some philosophers and cognitive scientists argue for vectorial representations of concepts, where the meaning of a word is represented as its position in a high-dimensional neural state space. At the intersection of natural language processing and artificial intelligence, a class of very successful distributional word vector models has developed that can account for classic EEG findings of language, that is, the ease versus difficulty of integrating a word with its sentence context. However, models of semantics have to account not only for context-based word processing, but should also describe how word meaning is represented. Here, we investigate whether distributional vector representations of word meaning can model brain activity induced by words presented without context. Using EEG activity (event-related brain potentials) collected while participants in two experiments (English and German) read isolated words, we encoded and decoded word vectors taken from the family of prediction-based Word2vec algorithms. We found that, first, the position of a word in vector space allows the prediction of the pattern of corresponding neural activity over time, in particular during a time window of 300 to 500 ms after word onset. Second, distributional models perform better than a human-created taxonomic baseline model (WordNet), and this holds for several distinct vector-based models. Third, multiple latent semantic dimensions of word meaning can be decoded from brain activity. Combined, these results suggest that empiricist, prediction-based vectorial representations of meaning are a viable candidate for the representational architecture of human semantic knowledge.},
	number = {1},
	urldate = {2024-07-01},
	journal = {Neurobiology of Language},
	author = {Sassenhagen, Jona and Fiebach, Christian J.},
	month = mar,
	year = {2020},
	pages = {54--76},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\PVMI7PXW\\Sassenhagen und Fiebach - 2020 - Traces of Meaning Itself Encoding Distributional .pdf:application/pdf;Snapshot:C\:\\Users\\felix\\Zotero\\storage\\CSAP8USW\\Traces-of-Meaning-Itself-Encoding-Distributional.html:text/html},
}

@misc{ootaSpeechLanguageModels2024,
	title = {Speech language models lack important brain-relevant semantics},
	url = {http://arxiv.org/abs/2311.04664},
	doi = {10.48550/arXiv.2311.04664},
	abstract = {Despite known differences between reading and listening in the brain, recent work has shown that text-based language models predict both text-evoked and speech-evoked brain activity to an impressive degree. This poses the question of what types of information language models truly predict in the brain. We investigate this question via a direct approach, in which we systematically remove specific low-level stimulus features (textual, speech, and visual) from language model representations to assess their impact on alignment with fMRI brain recordings during reading and listening. Comparing these findings with speech-based language models reveals starkly different effects of low-level features on brain alignment. While text-based models show reduced alignment in early sensory regions post-removal, they retain significant predictive power in late language regions. In contrast, speech-based models maintain strong alignment in early auditory regions even after feature removal but lose all predictive power in late language regions. These results suggest that speech-based models provide insights into additional information processed by early auditory regions, but caution is needed when using them to model processing in late language regions. We make our code publicly available. [https://github.com/subbareddy248/speech-llm-brain]},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Oota, Subba Reddy and Çelik, Emin and Deniz, Fatma and Toneva, Mariya},
	month = jun,
	year = {2024},
	note = {arXiv:2311.04664 [cs, eess, q-bio]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: 26 pages, 20 figures, The 62nd Annual Meeting of the Association for Computational Linguistics, Long paper - Main},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\2W5MRDIE\\Oota et al. - 2024 - Speech language models lack important brain-releva.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\SCRZEMZV\\2311.html:text/html},
}

@misc{nichollsCongruencyEffectsObject2024,
	title = {Congruency effects on object recognition persist when objects are placed in the wild: {An} {AR} and mobile {EEG} study},
	copyright = {© 2024, Posted by Cold Spring Harbor Laboratory. The copyright holder has placed this preprint in the Public Domain. It is no longer restricted by copyright. Anyone can legally share, reuse, remix, or adapt this material for any purpose without crediting the original authors.},
	shorttitle = {Congruency effects on object recognition persist when objects are placed in the wild},
	url = {https://www.biorxiv.org/content/10.1101/2024.05.30.596613v1},
	doi = {10.1101/2024.05.30.596613},
	abstract = {Objects in expected locations are recognised faster and more accurately than objects in incongruent environments. This congruency effect has a neural component, with increased activity for objects in incongruent environments. Studies have increasingly shown differences between neural processes in realistic environments and tasks, and neural processes in the laboratory. To what extent do findings obtained from a laboratory setting translate to neural processes elicited in real-world environments? We investigated how object recognition is modulated when objects are placed in real environments using augmented reality while recording mobile EEG. Participants approached, viewed, and rated how congruent they found the objects with the environment. We found significantly higher theta-band power for objects in incongruent contexts than objects in congruent contexts. This demonstrates that real-world contexts impact on how we recognize objects, and that mobile brain imaging and augmented reality are effective tools to study cognition in the wild.
Teaser Combining augmented reality with mobile brain imaging to show that real-world contexts modulate object recognition processes.},
	language = {en},
	urldate = {2024-07-01},
	publisher = {bioRxiv},
	author = {Nicholls, Victoria I. and Krugliak, Alexandra and Alsbury-Nealy, Benjamin and Gramann, Klaus and Clarke, Alex},
	month = may,
	year = {2024},
	note = {Pages: 2024.05.30.596613
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\felix\\Zotero\\storage\\B3TSX4P9\\Nicholls et al. - 2024 - Congruency effects on object recognition persist w.pdf:application/pdf},
}

@misc{vilasPositionPaperInner2024,
	title = {Position {Paper}: {An} {Inner} {Interpretability} {Framework} for {AI} {Inspired} by {Lessons} from {Cognitive} {Neuroscience}},
	shorttitle = {Position {Paper}},
	url = {http://arxiv.org/abs/2406.01352},
	doi = {10.48550/arXiv.2406.01352},
	abstract = {Inner Interpretability is a promising emerging field tasked with uncovering the inner mechanisms of AI systems, though how to develop these mechanistic theories is still much debated. Moreover, recent critiques raise issues that question its usefulness to advance the broader goals of AI. However, it has been overlooked that these issues resemble those that have been grappled with in another field: Cognitive Neuroscience. Here we draw the relevant connections and highlight lessons that can be transferred productively between fields. Based on these, we propose a general conceptual framework and give concrete methodological strategies for building mechanistic explanations in AI inner interpretability research. With this conceptual framework, Inner Interpretability can fend off critiques and position itself on a productive path to explain AI systems.},
	urldate = {2024-07-01},
	publisher = {arXiv},
	author = {Vilas, Martina G. and Adolfi, Federico and Poeppel, David and Roig, Gemma},
	month = jun,
	year = {2024},
	note = {arXiv:2406.01352 [cs, q-bio]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: Accepted at ICML 2024},
	file = {arXiv Fulltext PDF:C\:\\Users\\felix\\Zotero\\storage\\9GEK6QC8\\Vilas et al. - 2024 - Position Paper An Inner Interpretability Framewor.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\felix\\Zotero\\storage\\Y8TRPY4L\\2406.html:text/html},
}
